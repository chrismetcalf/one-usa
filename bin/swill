#!/usr/bin/env ruby
# EventMachine consumer for http://www.usa.gov/About/developer_resources/developers.shtml

require 'rubygems'
require 'em-http'
require 'json'
require 'thread'

['socrata'].each do |lib|
  require File.join(File.dirname(__FILE__), "../lib/#{lib}")
end

CONFIG = YAML.load_file(ARGV[0])[ENV["RACK_ENV"] || "development"]

EM.run do
  buffer = ""
  links = []
  links_semaphore = Mutex.new
  prev_batchsize = 0
  last_event = Time.now

  socrata = Socrata.new({
    :domain => CONFIG["domain"],
    :username => CONFIG["username"],
    :password => CONFIG["password"],
    :app_token => CONFIG["app_token"]
  })

  # Follow the bit.ly stream...
  http = EventMachine::HttpRequest.new(CONFIG["feed_url"]).get

  http.callback {
    if http.response_header.status == 200
      puts "Call succeeded, streaming..."
    else
      $stderr.puts "Call failed with response code #{http.response_header.status}"
      http = EventMachine::HttpRequest.new(CONFIG["feed_url"]).get
    end
  }

  http.stream do |chunk|
    buffer += chunk
    while line = buffer.slice!(/.+\r?\n/)
      begin
        link = JSON.parse(line.strip)
        if !link.nil?
          links_semaphore.synchronize {
            links << link

            # Store away our new batch size and timestamp, so we can track when we last saw
            # an event
            prev_batchsize = links.size
            last_event = Time.now
          }

        end
      rescue JSON::ParserError => e
        # Swallow your pride...
      end
    end
  end

  http.disconnect do
    $stderr.puts "Lost connection!!!!"
    http = EventMachine::HttpRequest.new(CONFIG["feed_url"]).get
  end

  # Periodically flush the queue to the API
  EM.add_periodic_timer(CONFIG["check_time"]) do
    links_semaphore.synchronize do
      if links.count < CONFIG["batch_size"]
        # Check to see if it's been too long with us stuck at this batch size
        if((links.count == 0 || links.count == prev_batchsize) && Time.now - last_event > CONFIG["stuck_time"])
          $stderr.puts "Connection stuck, committing sepuku..."
          exit 1
        end

        puts "Batch too small (#{links.count}), skipping..."
      else
        puts "Processing batch of #{links.count}..."
        begin
          socrata.batch do
            while link = links.shift
              payload = {
                "Short URL" => {"url" => "http://1.usa.gov/#{link["g"]}"},
                "User Agent" => link["a"],
                "Country Code" => link["c"],
                "Known User" => (link["nk"] == 1),
                "Global Bitly Hash" => link["g"],
                "User Bitly Hash" => link["h"],
                "User Login" => link["l"],
                "Short URL CNAME" => link["hh"],
                "Referring URL" => (link["r"] == "direct" ? nil : {"url" => link["r"]}),
                "Long URL" => {"url" => link["u"]},
                "Timestamp" => link["t"],
                "Geo Region" => link["gr"],
                "City" => link["cy"],
                "Timezone" => link["tz"],
                "Hash Timestamp" => link["hc"],
                "Language" => link["al"]
              }

              if !link["ll"].blank?
                payload["Location"] = {"latitude" => link["ll"][0], "longitude" => link["ll"][1]}
              end

              # Clear out null values
              payload = payload.delete_if {|k, v| v.nil?}

              #puts "New Row: #{payload.inspect}"
              socrata.post("/api/views/#{CONFIG["dataset"]}/rows.json", {:body => payload.to_json})
            end
          end

          # Reset last batch
          prev_batchsize = 0
          last_event = Time.now
        rescue Exception => e
          $stderr.puts "Error in batch: #{e}: #{e.backtrace.join("\n")}}"
        end
      end
    end
  end
end
